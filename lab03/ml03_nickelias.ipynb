{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elias Analytics Titanic Dataset ML Analysis\n",
    "**Author:** Nick Elias  \n",
    "**Date:** 3/28/25  \n",
    "**Objective:** To develop and evaluate machine learning models that predict passenger survival on the Titanic, while exploring the relationships between various passenger characteristics and survival rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Import and Inspect the Data\n",
    "In the code cell below, import the necessary Python libraries for this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Imports for the Titanic dataset '''\n",
    "\n",
    "# Import pandas for data manipulation and analysis (we might want to do more with it)\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Import pandas for data manipulation and analysis  (we might want to do more with it)\n",
    "import numpy as np\n",
    "\n",
    "# Import matplotlib for creating static visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import seaborn for statistical data visualization (built on matplotlib)\n",
    "import seaborn as sns\n",
    "\n",
    "# Import train_test_split for splitting data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Import LinearRegression for building a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import performance metrics for model evaluation\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['age'].fillna(titanic['age'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['embark_town'].fillna(titanic['embark_town'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Feature Engineering\n",
    "### New features:\n",
    "\n",
    "* Add family_size - number of family members on board\n",
    "* Convert categorical \"sex\" to numeric\n",
    "* Convert categorical \"embarked\" to numeric\n",
    "* Binary feature - convert \"alone\" to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
    "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "titanic['alone'] = titanic['alone'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Feature Selection and Justification\n",
    "\n",
    "### 3.1 Choose features and target\n",
    "* Select two or more input features (numerical for regression, numerical and/or categorical for classification)\n",
    "* Use survived as the target. \n",
    "* We will do three input cases like the example. \n",
    "\n",
    "### First:\n",
    "\n",
    "* input features: alone\n",
    "* target: survived\n",
    "\n",
    "### Second:\n",
    "\n",
    "* input features - age (or another variable of your choice)\n",
    "* target: survived\n",
    "\n",
    "### Third:\n",
    "\n",
    "* input features -  age and family_size (or another combination of your choice)\n",
    "* target: survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define X (features) and y (target)\n",
    "* Assign input features to X a pandas DataFrame with 1 or more input features\n",
    "* Assign target variable to y (as applicable) - a pandas Series with a single target feature\n",
    "* Again - use comments to run a single case at a time\n",
    "* The follow starts with only the statements needed for case 1. \n",
    "* Double brackets [[ ]]]  makes a 2D DataFrame\n",
    "* Single brackets [ ]  make a 1D Series\n",
    " \n",
    "\n",
    "### Case 1: alone only \n",
    "* X = titanic[['alone']]\n",
    "* y = titanic['survived']\n",
    "\n",
    "### Case 2: age only (or your choice)\n",
    "* X = titanic[['age']]\n",
    "* y = titanic['survived']\n",
    "\n",
    "### Case 3: age + family_size (or your choice)\n",
    "* X = titanic[['age', 'family_size']]\n",
    "* y = titanic['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection 3:\n",
    "\n",
    "* Why are these features selected?\n",
    "* Are there features that are likely to be highly predictive of survival?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Train a Classification Model (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Split the Data\n",
    "Split the data into training and test sets. Use StratifiedShuffleSplit to ensure even class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=123)\n",
    "\n",
    "for train_indices, test_indices in splitter.split(X, y):\n",
    "    X_train = X.iloc[train_indices]\n",
    "    X_test = X.iloc[test_indices]\n",
    "    y_train = y.iloc[train_indices]\n",
    "    y_test = y.iloc[test_indices]\n",
    "\n",
    "print('Train size: ', len(X_train), 'Test size: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .2 Create and Train Model (Decision Tree)\n",
    "Create and train a decision tree model with no random initializer argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance on test data:\n",
    "\n",
    "# Predict and evaluate test data\n",
    "y_test_pred = tree_model.predict(X_test)\n",
    "print(\"Results for Decision Tree on test data:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Report Confusion Matrix (as a heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Report Decision Tree Plot\n",
    "Plot the decision tree model. We give the plotter the names of the features and the names of the categories for the target. Save the image so we can use it in other places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plot_tree(tree_model, feature_names=X.columns, class_names=['Not Survived', 'Survived'], filled=True)\n",
    "plt.show()\n",
    "fig.savefig(\"decision_tree_titanic.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for All 3 Cases\n",
    "Try this for the 3 different cases: \n",
    "1) using height as the only input  \n",
    "2) using weight as the only input \n",
    "3) using height and weight together as inputs. \n",
    "\n",
    "For each different case, redefine the input features in Section 3 (comment out the old case inputs X and target y and uncomment the new case inputs X and target y), then re-run Sections 4 and 5 for each case. Record your results in a Markdown table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection 4:\n",
    "* How well did the different cases perform?\n",
    "* Are there any surprising results?\n",
    "* Which inputs worked better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Compare Alternative Models (SVC, NN)\n",
    "\n",
    "In a Support Vector Machine, the kernel function defines how the algorithm transforms data to find a hyperplane that separates the classes. If the data is not linearly separable, changing the kernel can help the model find a better decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Kernel: Common Types\n",
    "\n",
    "* RBF (Radial Basis Function) – Most commonly used; handles non-linear data well (default)\n",
    "* Linear – Best for linearly separable data (straight line separation)\n",
    "* Polynomial – Useful when the data follows a curved pattern\n",
    "* Sigmoid – Similar to a neural network activation function; less common\n",
    "Commenting the options in and out in the code can be helpful. The analyst decides which to use based on their understanding of the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF Kernel (default) - same as calling SVC()\n",
    "svc_model = SVC(kernel='rbf')\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Linear Kernel\n",
    "svc_model = SVC(kernel='linear')\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Polynomial Kernel (e.g., with degree=3)\n",
    "svc_model = SVC(kernel='poly', degree=3)\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Sigmoid Kernel\n",
    "svc_model = SVC(kernel='sigmoid')\n",
    "svc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Kernel: How to Choose\n",
    "\n",
    "* Start with linear if you suspect the data is linearly separable.\n",
    "* Use RBF if the data is complex or you aren’t sure.\n",
    "* Try polynomial if the data seems to follow a curved boundary.\n",
    "* Use sigmoid for experiments (rarely the best choice).\n",
    "\n",
    "### SVC Kernel: Common Issues and Techniques\n",
    "\n",
    "* If the model takes too long to train, reduce the degree for polynomial kernels.\n",
    "* If support_vectors_ gives an error, the data may not be separable with the current kernel. Try switching to RBF or adjusting the C (regularization) value.\n",
    "* If the model misclassifies many points, then increase/decrease gamma or C.\n",
    "Your process is valuable - record the work you do and the temporary results in your reflections and insights. To show your skills, show and tell us about your analysis process. Professional communication is key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Train and Evaluate Model (SVC)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, train an SVC model using the default kernel.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate the SVC model:\n",
    "\n",
    "y_pred_svc = svc_model.predict(X_test)\n",
    "\n",
    "print(\"Results for SVC on test data:\")\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Support Vectors\n",
    "Create a scatter plot to visualize the support vectors. This helps understand how the SVM model separates the data.\n",
    "\n",
    "### Step-by-Step Process:\n",
    "\n",
    "1) Split the data into two groups:\n",
    "\n",
    "* Survived – Passengers who survived the Titanic sinking (value = 1).\n",
    "* Not Survived – Passengers who did not survive (value = 0).\n",
    "2) Create a scatter plot of these two groups using different colors and markers:\n",
    "\n",
    "* Yellow squares ('s') for survived passengers\n",
    "* Cyan triangles ('^') for non-survived passengers\n",
    "3) Overlay the support vectors on top of the plot:\n",
    "\n",
    "* Black pluses ('+') will represent the support vectors.\n",
    "* Since the support vectors are plotted last, they appear on top of the data points and are not obscured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create data for charting - input target yes and input target no\n",
    "survived_alone = X_test.loc[y_test == 1, 'alone']\n",
    "not_survived_alone = X_test.loc[y_test == 0, 'alone']\n",
    "\n",
    "# Create scatter plot for survived and not survived\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(survived_alone, y_test.loc[y_test == 1], c='yellow', marker='s', label='Survived')\n",
    "plt.scatter(not_survived_alone, y_test.loc[y_test == 0], c='cyan', marker='^', label='Not Survived')\n",
    "\n",
    "# Add support vectors (if available)\n",
    "if hasattr(svc_model, 'support_vectors_'):\n",
    "    support_x = svc_model.support_vectors_[:, 0]  # First feature (alone)\n",
    "    support_y = svc_model.support_vectors_[:, 1] if svc_model.support_vectors_.shape[1] > 1 else None\n",
    "    \n",
    "    # Plot support vectors\n",
    "    if support_y is not None:\n",
    "        plt.scatter(support_x, support_y, c='black', marker='+', label='Support Vectors')\n",
    "    else:\n",
    "        plt.scatter(support_x, [0] * len(support_x), c='black', marker='+', label='Support Vectors')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Alone')\n",
    "plt.ylabel('Survived')\n",
    "plt.legend()\n",
    "plt.title('Support Vectors (SVC)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:  The support_vectors_ attribute might give an error if the model didn't converge or if the problem is not linearly separable. To try to get it to converge, try adjusting the kernel or tuning hyperparameters (more on this below). \n",
    "\n",
    "Adjust the charting accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Train and Evaluate Model (NN MLP)\n",
    "\n",
    "Now we'll use the NN (Multi Level Perceptron ) model. Again, we will give the neural net as much information as possible and understand that it could overfit on the extra data.\n",
    "\n",
    "We have some hyper parameters that we can adjust. For the other models we just let them run with their defaults. Here we are going to use 3 hidden layers and change up the solver to one that is more likely to give good results for a small data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a neural network model:\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "nn_model = MLPClassifier(hidden_layer_sizes=(50, 25, 10), solver='lbfgs')\n",
    "nn_model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate Neural Network model:\n",
    "\n",
    "y_pred_nn = nn_model.predict(X_test)\n",
    "\n",
    "print(\"Results for Neural Network on test data:\")\n",
    "print(classification_report(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix:\n",
    "\n",
    "cm_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "sns.heatmap(cm_nn, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection 5:\n",
    "* How well did each model perform?\n",
    "* Are there any surprising results?\n",
    "* Why might one model outperform the others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6. Final Thoughts & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Summarize Findings\n",
    "* What indicators are strong predictors of gender?\n",
    "* Decision Tree performed well but overfit slightly on training data.\n",
    "* Neural Network showed moderate improvement but introduced complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Discuss Challenges Faced\n",
    "* Small sample size could limit generalizability.\n",
    "* Missing values (if any) could bias the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Next Steps\n",
    "* Test more features (e.g., BMI class).\n",
    "* So Try hyperparameter tuning for better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
